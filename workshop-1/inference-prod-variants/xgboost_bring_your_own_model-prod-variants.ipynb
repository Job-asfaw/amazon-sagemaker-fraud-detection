{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker XGBoost Bring Your Own Model\n",
    "_**Hosting a Pre-Trained scikit-learn Model in Amazon SageMaker XGBoost Algorithm Container**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Optionally, train a scikit learn XGBoost model](#Optionally,-train-a-scikit-learn-XGBoost-model)\n",
    "1. [Upload the pre-trained model to S3](#Upload-the-pre-trained-model-to-S3)\n",
    "1. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "1. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker includes functionality to support a hosted notebook environment, distributed, serverless training, and real-time hosting. We think it works best when all three of these services are used together, but they can also be used independently.  Some use cases may only require hosting.  Maybe the model was trained prior to Amazon SageMaker existing, in a different service.\n",
    "\n",
    "This notebook shows how to use a pre-existing scikit-learn trained XGBoost model with the Amazon SageMaker XGBoost Algorithm container to quickly create a hosted endpoint for that model. Please note that scikit-learn XGBoost model is compatible with SageMaker XGBoost container, whereas other gradient boosted tree models (such as one trained in SparkML) are not.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* AWS region.\n",
    "* The IAM role arn used to give learning and hosting access to your data. See the documentation for how to specify these.\n",
    "* The S3 bucket that you want to use for training and model data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker', endpoint_url= 'https://vpce-0803dc74776014d46-iava3t25-us-west-2a.vpce-svc-00a9fd01107f2b64d.us-west-2.vpce.amazonaws.com')\n",
    "\n",
    "#runtime_client = boto3.client('runtime.sagemaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 129 ms, total: 1.13 s\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "bucket=sess.default_bucket()\n",
    "#bucket='<s3 bucket>' # put your s3 bucket name here, and create s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "prefix = 'sagemaker/DEMO-xgboost-byo'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "# customize to your bucket where you have stored the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally, train a scikit learn XGBoost model\n",
    "\n",
    "These steps are optional and are needed to generate the scikit-learn model that will eventually be hosted using the SageMaker Algorithm contained. \n",
    "\n",
    "### Install XGboost\n",
    "Note that for conda based installation, you'll need to change the Notebook kernel to the environment with conda and Python3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/91/551d37ba472bcbd70a25e667acc65a18a9d053657b13afcf0f87aa24d7bb/xgboost-1.0.2-py3-none-manylinux1_x86_64.whl (109.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 109.8MB 429kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.14.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.1.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 900 ms, sys: 258 ms, total: 1.16 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 12.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "def get_dataset():\n",
    "  import pickle\n",
    "  import gzip\n",
    "  with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "      u = pickle._Unpickler(f)\n",
    "      u.encoding = 'latin1'\n",
    "      return u.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = get_dataset()\n",
    "\n",
    "train_X = train_set[0]\n",
    "train_y = train_set[1]\n",
    "\n",
    "valid_X = valid_set[0]\n",
    "valid_y = valid_set[1]\n",
    "\n",
    "test_X = test_set[0]\n",
    "test_y = test_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/config.py:63: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config.update(yaml.load(text) or {})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.2, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=10, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method=None,\n",
       "       validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn as sk \n",
    "\n",
    "bt = xgb.XGBClassifier(max_depth=5,\n",
    "                       learning_rate=0.2,\n",
    "                       n_estimators=10,\n",
    "                       objective='multi:softmax')   # Setup xgboost model\n",
    "#900597767885.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:beta_low_lat_inf\n",
    "\n",
    "\n",
    "bt.fit(train_X, train_y, # Train it to our data\n",
    "       eval_set=[(valid_X, valid_y)], \n",
    "       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'multi:softprob',\n",
       " 'base_score': 0.5,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': -1,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.2,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': 0,\n",
       " 'num_parallel_tree': 1,\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 1,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': False,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt.get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model file\n",
    "Note that the model file name must satisfy the regular expression pattern: `^[a-zA-Z0-9](-*[a-zA-Z0-9])*;`. The model file also need to tar-zipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"xgboost-model-\"\n",
    "bt._Booster.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost-model-\r\n"
     ]
    }
   ],
   "source": [
    "!tar czvf model.tar.gz $model_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pre-trained model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fObj = open(\"model.tar.gz\", 'rb')\n",
    "key= os.path.join(prefix, model_file_name, 'model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model in your VPC/Subnet with Security Groups\n",
    "\n",
    "### Import model into hosting\n",
    "This involves creating a SageMaker model from the model file previously uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image: 257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "#container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "#container = '900597767885.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:beta_low_lat_inf'\n",
    "print('container image: {}'.format(container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_url: https://s3-us-east-2.amazonaws.com/sagemaker-us-east-2-328296961357/sagemaker/DEMO-xgboost-byo/xgboost-model-/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-2:328296961357:model/xgboost-model-2020-03-19-01-50-07\n",
      "xgboost-model-2020-03-19-01-50-07\n",
      "CPU times: user 52.8 ms, sys: 7.89 ms, total: 60.7 ms\n",
      "Wall time: 434 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = model_file_name + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 'https://s3-{}.amazonaws.com/{}/{}'.format(region,bucket,key)\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print ('model_url: {}'.format(model_url))\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url\n",
    "}\n",
    "\n",
    "#vpc = 'vpc-0e3dc7a6ecb4b94c0'\n",
    "#subnet_a = 'subnet-018d48710058a32fb'\n",
    "#subnet_b = 'subnet-06d5f220180ba3862'\n",
    "#security_group = 'sg-01ac05ba3308465ac'\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container\n",
    "#    ,\n",
    "#    VpcConfig={\n",
    "#        'SecurityGroupIds': [security_group],\n",
    "#        'Subnets': [subnet_a, subnet_b],\n",
    "#    },\n",
    "#    EnableNetworkIsolation=True\n",
    "    )\n",
    "\n",
    "print(create_model_response['ModelArn'])\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration for a single model\n",
    "\n",
    "SageMaker supports configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, you can create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way. In addition, the endpoint configuration describes the instance type required for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostEP-Prod-2020-03-19-01-50-07\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-2:328296961357:endpoint-config/xgboostep-prod-2020-03-19-01-50-07\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name_prod = 'XGBoostEP-Prod-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name_prod)\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name_prod,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'model-Blue'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "You create the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 8-10 minutes to complete. \n",
    "\n",
    "#### First, let us create a helper funciton to poll for endoint status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_endpoint(endpoint_name, endpoint_ARN, poll_period=60):\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "    while (status=='Creating' or status=='Updating'):\n",
    "        time.sleep(poll_period)\n",
    "        resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = resp['EndpointStatus']\n",
    "        print(\"Status: \" + status)\n",
    "\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Status: \" + status)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let us create the endpoint and poll until it's status changes to \"InService\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostEP-2020-03-19-01-50-07\n",
      "arn:aws:sagemaker:us-east-2:328296961357:endpoint/xgboostep-2020-03-19-01-50-07\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-2:328296961357:endpoint/xgboostep-2020-03-19-01-50-07\n",
      "Status: InService\n",
      "CPU times: user 97 ms, sys: 4.19 ms, total: 101 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'XGBoostEP-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name_prod)\n",
    "endpoint_ARN = create_endpoint_response['EndpointArn']\n",
    "print(endpoint_ARN)\n",
    "\n",
    "poll_endpoint(endpoint_name=endpoint_name, endpoint_ARN=endpoint_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us seamlessly deploy a new model to the same endpoint without taking off-line the production 'model-Blue'. \n",
    "First we create a new endpoint configuraiton with both 'Blue' and 'Green' models. To save time, we are going to use the same model 'model_name' for both, but in real Blue/Green deployment these would be two different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostEP-BlueGreen2-2020-03-19-01-56-08\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-2:328296961357:endpoint-config/xgboostep-bluegreen2-2020-03-19-01-56-08\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name_blue_green = 'XGBoostEP-BlueGreen2-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name_blue_green)\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name_blue_green,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':0.1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'model-Blue'},\n",
    "        {'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':0.2,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'model-Green'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n",
    "#note that the weights do not need to add up to 1. In the above configuration, \n",
    "#Green model is going to be invoked twice as often as Blue one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update endpoint with Blue and Green models\n",
    "While this operation is in progress, the endpoint can still receive and respond to inference requests using previously deployed model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostEP-2020-03-19-01-50-07\n",
      "arn:aws:sagemaker:us-east-2:328296961357:endpoint/xgboostep-2020-03-19-01-50-07\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-2:328296961357:endpoint/xgboostep-2020-03-19-01-50-07\n",
      "Status: InService\n",
      "CPU times: user 214 ms, sys: 4.75 ms, total: 219 ms\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "#endpoint_name = 'XGBoostEP-BlueGreen-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "update_endpoint_response = sm_client.update_endpoint(\n",
    "    EndpointConfigName = endpoint_config_name_blue_green,\n",
    "    EndpointName = endpoint_name)\n",
    "endpoint_ARN = update_endpoint_response['EndpointArn']\n",
    "print(endpoint_ARN)\n",
    "\n",
    "poll_endpoint(endpoint_name=endpoint_name, endpoint_ARN=endpoint_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Endpoint's ProductionVariants Weights \n",
    "Assuming we are happy with \"Green\" model performance, let us switch all the traffic to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostEP-2020-03-19-01-50-07\n",
      "arn:aws:sagemaker:us-east-2:328296961357:endpoint/xgboostep-2020-03-19-01-50-07\n",
      "Status: Updating\n",
      "Status: Updating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-2:328296961357:endpoint/xgboostep-2020-03-19-01-50-07\n",
      "Status: InService\n",
      "CPU times: user 67.6 ms, sys: 436 µs, total: 68.1 ms\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(endpoint_name)\n",
    "update_weights_response = sm_client.update_endpoint_weights_and_capacities(    \n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            'VariantName': 'model-Blue',\n",
    "            'DesiredWeight': 0\n",
    "        },\n",
    "        {\n",
    "            'VariantName': 'model-Green',\n",
    "            'DesiredWeight': 1\n",
    "        }        \n",
    "    ]\n",
    ")\n",
    "\n",
    "endpoint_ARN = update_weights_response['EndpointArn']\n",
    "print(endpoint_ARN)\n",
    "\n",
    "poll_endpoint(endpoint_name=endpoint_name, endpoint_ARN=endpoint_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Now you can obtain the endpoint from the client library using the result from previous operations and generate classifications from the model using that endpoint. You can take the below code and use it as a basis for endpoint invocation from a utility EC2 machine or a different SageMaker notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate the prediction for a single datapoint. We'll pick one from the test data generated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "point_X = test_X[0]\n",
    "point_X = np.expand_dims(point_X, axis=0)\n",
    "point_y = test_y[0]\n",
    "np.savetxt(\"test_point.csv\", point_X, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '39460dc9-b903-4832-bf01-31586c296727', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '39460dc9-b903-4832-bf01-31586c296727', 'x-amzn-invoked-production-variant': 'model-Green', 'date': 'Thu, 19 Mar 2020 02:05:11 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '2'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'model-Green', 'Body': <botocore.response.StreamingBody object at 0x7fbe9020be48>}\n",
      "Predicted Class Probabilities: [].\n",
      "CPU times: user 13.1 ms, sys: 7 µs, total: 13.1 ms\n",
      "Wall time: 27.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "\n",
    "file_name = 'test_point.csv' #customize to your test file, will be 'mnist.single.test' if use data above\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "print (response)\n",
    "result = response['Body'].read().decode('ascii')\n",
    "print('Predicted Class Probabilities: {}.'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "\n",
    "If you're ready to be done with this notebook, please run the delete_endpoint line in the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
