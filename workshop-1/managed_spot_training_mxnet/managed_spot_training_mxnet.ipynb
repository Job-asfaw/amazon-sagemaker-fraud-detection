{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SageMaker Models using the Apache MXNet Module API on SageMaker Managed Spot Training\n",
    "\n",
    "The example here is almost the same as [Training and hosting SageMaker Models using the Apache MXNet Module API](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_mnist/mxnet_mnist.ipynb).\n",
    "\n",
    "This notebook tackles the exact same problem with the same solution, but it has been modified to be able to run using SageMaker Managed Spot infrastructure. SageMaker Managed Spot uses [EC2 Spot Instances](https://aws.amazon.com/ec2/spot/) to run Training at a lower cost.\n",
    "\n",
    "Please read the original notebook and try it out to gain an understanding of the ML use-case and how it is being solved. We will not delve into that here in this notebook.\n",
    "\n",
    "## First setup variables and define functions\n",
    "\n",
    "Again, we won't go into detail explaining the code below, it has been lifted verbatim from [Training and hosting SageMaker Models using the Apache MXNet Module API](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_mnist/mxnet_mnist.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = 's3://{}/customcode/mxnet'.format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = 's3://{}/artifacts'.format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()\n",
    "\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "train_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/train'.format(region)\n",
    "test_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/test'.format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Spot Training with MXNet\n",
    "\n",
    "For Managed Spot Training using MXNet we need to configure three things:\n",
    "1. Enable the `train_use_spot_instances` constructor arg - a simple self-explanatory boolean.\n",
    "2. Set the `train_max_wait` constructor arg - this is an int arg representing the amount of time you are willing to wait for Spot infrastructure to become available. Some instance types are harder to get at Spot prices and you may have to wait longer. You are not charged for time spent waiting for Spot infrastructure to become available, you're only charged for actual compute time spent once Spot instances have been successfully procured.\n",
    "3. Setup a `checkpoint_s3_uri` constructor arg. This arg will tell SageMaker an S3 location where to save checkpoints (assuming your algorithm has been modified to save checkpoints periodically). While not strictly necessary checkpointing is highly recommended for Manage Spot Training jobs due to the fact that Spot instances can be interrupted with short notice and using checkpoints to resume from the last interruption ensures you don't lose any progress made before the interruption.\n",
    "\n",
    "Feel free to toggle the `train_use_spot_instances` variable to see the effect of running the same job using regular (a.k.a. \"On Demand\") infrastructure.\n",
    "\n",
    "Note that `train_max_wait` can be set if and only if `train_use_spot_instances` is enabled and **must** be greater than or equal to `train_max_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=3600\n",
    "train_max_wait = 7200 if train_use_spot_instances else None\n",
    "import uuid\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = 's3://{}/artifacts/mxnet-checkpoint-{}/'.format(bucket, checkpoint_suffix) if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-22 06:31:52 Starting - Starting the training job...\n",
      "2020-01-22 06:31:54 Starting - Launching requested ML instances...\n",
      "2020-01-22 06:32:50 Starting - Preparing the instances for training......\n",
      "2020-01-22 06:33:52 Downloading - Downloading input data\n",
      "2020-01-22 06:33:52 Training - Downloading the training image..\u001b[34m2020-01-22 06:34:05,236 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:05,239 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:05,254 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"learning-rate\":0.1}', 'SM_USER_ENTRY_POINT': 'mnist.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_parameter_server_enabled\":true}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"test\",\"train\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'mnist', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-811346863928/customcode/mxnet/mxnet-training-2020-01-22-06-31-52-348/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning-rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-01-22-06-31-52-348\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-811346863928/customcode/mxnet/mxnet-training-2020-01-22-06-31-52-348/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}', 'SM_USER_ARGS': '[\"--learning-rate\",\"0.1\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train', 'SM_HP_LEARNING-RATE': '0.1'}\u001b[0m\n",
      "\n",
      "2020-01-22 06:34:04 Training - Training image download completed. Training in progress.\u001b[34m2020-01-22 06:34:11,472 sagemaker_mxnet_container.training INFO     Starting distributed training task\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:11,796 sagemaker-containers INFO     Module mnist does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:11,796 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:11,796 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:11,796 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mInstalling collected packages: mnist\n",
      "  Running setup.py install for mnist: started\n",
      "    Running setup.py install for mnist: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed mnist-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.1.1, however version 20.0.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:14,025 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:14,042 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning-rate\": 0.1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2020-01-22-06-31-52-348\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-811346863928/customcode/mxnet/mxnet-training-2020-01-22-06-31-52-348/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning-rate\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-811346863928/customcode/mxnet/mxnet-training-2020-01-22-06-31-52-348/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning-rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-01-22-06-31-52-348\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-811346863928/customcode/mxnet/mxnet-training-2020-01-22-06-31-52-348/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning-rate\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m mnist --learning-rate 0.1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [0-100]#011Speed: 48333.84 samples/sec#011accuracy=0.104455\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [100-200]#011Speed: 60725.94 samples/sec#011accuracy=0.115700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [200-300]#011Speed: 60823.95 samples/sec#011accuracy=0.112400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [300-400]#011Speed: 53524.58 samples/sec#011accuracy=0.116900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Batch [400-500]#011Speed: 55547.07 samples/sec#011accuracy=0.115800\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Train-accuracy=0.135800\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Time cost=1.104\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0001.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0001.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[0] Validation-accuracy=0.344700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [0-100]#011Speed: 45055.76 samples/sec#011accuracy=0.474554\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [100-200]#011Speed: 54867.02 samples/sec#011accuracy=0.653300\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [200-300]#011Speed: 57785.20 samples/sec#011accuracy=0.771000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [300-400]#011Speed: 53864.77 samples/sec#011accuracy=0.798000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Batch [400-500]#011Speed: 52844.69 samples/sec#011accuracy=0.825600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Train-accuracy=0.727350\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Time cost=1.139\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0002.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0002.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[1] Validation-accuracy=0.839200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [0-100]#011Speed: 46121.26 samples/sec#011accuracy=0.865644\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [100-200]#011Speed: 60427.78 samples/sec#011accuracy=0.869600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [200-300]#011Speed: 48717.22 samples/sec#011accuracy=0.886900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [300-400]#011Speed: 43225.99 samples/sec#011accuracy=0.895900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Batch [400-500]#011Speed: 52790.81 samples/sec#011accuracy=0.898300\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Train-accuracy=0.888217\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Time cost=1.197\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0003.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0003.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[2] Validation-accuracy=0.916100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO:root:Epoch[3] Batch [0-100]#011Speed: 51772.20 samples/sec#011accuracy=0.917228\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Batch [100-200]#011Speed: 55756.63 samples/sec#011accuracy=0.928500\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Batch [200-300]#011Speed: 53998.19 samples/sec#011accuracy=0.926900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Batch [300-400]#011Speed: 57517.52 samples/sec#011accuracy=0.930100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Batch [400-500]#011Speed: 53652.96 samples/sec#011accuracy=0.933300\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Train-accuracy=0.929817\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Time cost=1.119\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0004.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0004.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[3] Validation-accuracy=0.941500\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [0-100]#011Speed: 48999.51 samples/sec#011accuracy=0.943069\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [100-200]#011Speed: 52805.04 samples/sec#011accuracy=0.941000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [200-300]#011Speed: 52221.81 samples/sec#011accuracy=0.947400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [300-400]#011Speed: 65634.45 samples/sec#011accuracy=0.947100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Batch [400-500]#011Speed: 59161.03 samples/sec#011accuracy=0.951900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Train-accuracy=0.946750\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Time cost=1.082\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0005.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0005.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[4] Validation-accuracy=0.952900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [0-100]#011Speed: 48288.76 samples/sec#011accuracy=0.954851\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [100-200]#011Speed: 55078.42 samples/sec#011accuracy=0.955000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [200-300]#011Speed: 54548.84 samples/sec#011accuracy=0.955600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [300-400]#011Speed: 55253.57 samples/sec#011accuracy=0.959800\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Batch [400-500]#011Speed: 57525.41 samples/sec#011accuracy=0.958600\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Train-accuracy=0.957550\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Time cost=1.111\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0006.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0006.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[5] Validation-accuracy=0.957400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [0-100]#011Speed: 48358.02 samples/sec#011accuracy=0.960594\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [100-200]#011Speed: 54126.06 samples/sec#011accuracy=0.964200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [200-300]#011Speed: 54800.57 samples/sec#011accuracy=0.964400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [300-400]#011Speed: 55918.24 samples/sec#011accuracy=0.962900\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Batch [400-500]#011Speed: 56403.71 samples/sec#011accuracy=0.965500\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Train-accuracy=0.964100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Time cost=1.114\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0007.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0007.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[6] Validation-accuracy=0.961100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [0-100]#011Speed: 47264.66 samples/sec#011accuracy=0.969208\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [100-200]#011Speed: 60451.20 samples/sec#011accuracy=0.969100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [200-300]#011Speed: 52420.41 samples/sec#011accuracy=0.968100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [300-400]#011Speed: 49875.72 samples/sec#011accuracy=0.969200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Batch [400-500]#011Speed: 53185.56 samples/sec#011accuracy=0.969800\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Train-accuracy=0.968883\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Time cost=1.148\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0008.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0008.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[7] Validation-accuracy=0.962200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [0-100]#011Speed: 45453.26 samples/sec#011accuracy=0.972178\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [100-200]#011Speed: 54337.54 samples/sec#011accuracy=0.973100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [200-300]#011Speed: 60324.01 samples/sec#011accuracy=0.972400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [300-400]#011Speed: 58143.43 samples/sec#011accuracy=0.972100\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Batch [400-500]#011Speed: 55235.74 samples/sec#011accuracy=0.973000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Train-accuracy=0.972450\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Time cost=1.116\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0009.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0009.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[8] Validation-accuracy=0.967700\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [0-100]#011Speed: 38484.87 samples/sec#011accuracy=0.980495\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [100-200]#011Speed: 51828.37 samples/sec#011accuracy=0.973000\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [200-300]#011Speed: 53764.72 samples/sec#011accuracy=0.976200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [300-400]#011Speed: 55350.99 samples/sec#011accuracy=0.974200\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Batch [400-500]#011Speed: 55912.20 samples/sec#011accuracy=0.976400\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Train-accuracy=0.976167\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Time cost=1.204\u001b[0m\n",
      "\u001b[34mINFO:root:Saved checkpoint to \"/opt/ml/checkpoints/mnist-0010.params\"\u001b[0m\n",
      "\u001b[34mINFO:root:Saved optimizer state to \"/opt/ml/checkpoints/mnist-0010.states\"\u001b[0m\n",
      "\u001b[34mINFO:root:Epoch[9] Validation-accuracy=0.969700\u001b[0m\n",
      "\u001b[34m2020-01-22 06:34:31,957 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-22 06:34:42 Uploading - Uploading generated training model\n",
      "2020-01-22 06:34:42 Completed - Training job completed\n",
      "Training seconds: 66\n",
      "Billable seconds: 13\n",
      "Managed Spot Training savings: 80.3%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "mnist_estimator = MXNet(entry_point='mnist.py',\n",
    "                        role=role,\n",
    "                        output_path=model_artifacts_location,\n",
    "                        code_location=custom_code_upload_location,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.m4.xlarge',\n",
    "                        framework_version='1.4.1',\n",
    "                        py_version='py3',\n",
    "                        distributions={'parameter_server': {'enabled': True}},\n",
    "                        hyperparameters={'learning-rate': 0.1},\n",
    "                        train_use_spot_instances=train_use_spot_instances,\n",
    "                        train_max_run=train_max_run,\n",
    "                        train_max_wait=train_max_wait,\n",
    "                        checkpoint_s3_uri=checkpoint_s3_uri)\n",
    "mnist_estimator.fit({'train': train_data_location, 'test': test_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Savings\n",
    "Towards the end of the job you should see two lines of output printed:\n",
    "\n",
    "- `Training seconds: X` : This is the actual compute-time your training job spent\n",
    "- `Billable seconds: Y` : This is the time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "If you enabled the `train_use_spot_instances` var then you should see a notable difference between `X` and `Y` signifying the cost savings you will get for having chosen Managed Spot Training. This should be reflected in an additional line:\n",
    "- `Managed Spot Training savings: (1-Y/X)*100 %`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
