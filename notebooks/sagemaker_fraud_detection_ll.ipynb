{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card fraud detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading and reading in the credit card fraud data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://s3-us-west-2.amazonaws.com/sagemaker-e2e-solutions/fraud-detection/creditcardfraud.zip\n",
    "unzip creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('creditcard.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our data (we only show a subset of the columns in the table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "data[['Time', 'V1', 'V2', 'V27', 'V28', 'Amount', 'Class']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulant with only $492$ ($.173\\%$) of the data corresponding to fraudulant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfrauds, frauds = data.groupby('Class').size()\n",
    "print('Number of frauds: ', frauds)\n",
    "print('Number of non-frauds: ', nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*frauds/(frauds + nonfrauds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 28 columns, $V_i$ for $i=1..28$ of anonymized features along with columns for time, amount, and class. We already know that the columns $V_i$ have been normalized to have $0$ mean and unit standard deviation as the result of a PCA. You can read more about PCA here:. \n",
    "\n",
    "Tip: For our dataset this amount of preprocessing will give us reasonable accuracy, but it's important to note that there are more preprocessing steps one can use to improve accuracy . For unbalanced data sets like ours where the positive (fraudulent) examples occur much less frequently than the negative (legitimate) examples, we may try “over-sampling” the minority dataset by generating synthetic data (read about SMOTE in Data Mining for Imbalanced Datasets: An Overview (https://link.springer.com/chapter/10.1007%2F0-387-25465-X_40) or undersampling the majority class by using ensemble methods (see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&rep=rep1&type=pdfor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = data.columns[:-1]\n",
    "label_column = data.columns[-1]\n",
    "\n",
    "features = data[feature_columns].values.astype('float32')\n",
    "labels = (data[label_column].values).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some analysis and discuss different ways we can preprocess our data. Let's discuss the way in which this data was preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Linear Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon common libraries provide utilities to convert NumPy n-dimensional arrays into a the Record-IO format which SageMaker uses for a concise representation of features and labels. The Record-IO format is implemented via protocol buffer so the serialization is very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, features, labels)\n",
    "buf.seek(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we upload the data to S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "#sagemaker_iam_role = get_execution_role()\n",
    "\n",
    "prefix = 'linear-learner'\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a Linear Learner using SageMaker's built-in algorithm. To specify the Linear Learner algorithm, we use a utility function to obtain it's URI. A complete list of build-in algorithms is found here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker abstracts training with Estimators. We can pass container, and all parameters to the estimator, as well as the hyperparameters for the linear learner and fit the estimator to the data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role = get_execution_role(), \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c5.4xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=session)\n",
    "linear.set_hyperparameters(feature_dim=features.shape[1],\n",
    "                           predictor_type='binary_classifier',\n",
    "                           mini_batch_size=200)\n",
    "\n",
    "linear.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the estimator to and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                 endpoint_name=\"fraud-detection-endpoint-c5p18xl\",\n",
    "                                 instance_type='ml.c5.xlarge') #'ml.c5.18xlarge'\n",
    "# Specify input and output formats.\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We will leave the prediction endpoint running at the end of this notebook so we can handle incoming event streams. However, don't forget to delete the prediction endpoint when you're done. You can do that at the Amazon SageMaker console in the Endpoints page. Or you can run `linear_predictor.delete_endpoint()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Acknowledgements\n",
    "\n",
    "The dataset used to demonstrated the fraud detection solution has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the [DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/) project\n",
    "We cite the following works:\n",
    "* Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "* Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "* Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "* Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "* Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "* Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sagemaker_endpoint_name = 'fraud-detection-endpoint-c5p18xl' \n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "tic = time.time()\n",
    "response = sagemaker_runtime.invoke_endpoint(EndpointName=sagemaker_endpoint_name, \n",
    "                    ContentType='text/csv', Body=data)\n",
    "toc = time.time()\n",
    "print(\"invocation latency\", toc-tic)\n",
    "print(response)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)\n",
    "pred = int(result['predictions'][0]['predicted_label'])\n",
    "return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    data_payload = get_data(event, context)\n",
    "    if not data_payload:\n",
    "        return\n",
    "    tic = time.time()\n",
    "    pred = get_fraud_prediction(data_payload)\n",
    "    toc = time.time()\n",
    "    print(\"prediction + postprocessing latency\", toc-tic)\n",
    "    transformed_data = postprocess_event(event, pred)\n",
    "    response = store_data_prediction(transformed_data)\n",
    "    print(response)\n",
    "\n",
    "def get_data(event, context):\n",
    "    if random.random() < 0.15:\n",
    "        return\n",
    "    non_fraud_example = [1.00000000e+00, -9.66271698e-01, -1.85226008e-01, 1.79299331e+00, -8.63291264e-01, -1.03088794e-02, 1.24720311e+00, 2.37608939e-01,\n",
    "                         3.77435863e-01, -1.38702404e+00, -5.49519211e-02, -2.26487264e-01, 1.78228229e-01, 5.07756889e-01, -2.87923753e-01, -6.31418109e-01,\n",
    "                         -1.05964720e+00, -6.84092760e-01, 1.96577501e+00, -1.23262203e+00, -2.08037779e-01, -1.08300455e-01, 5.27359685e-03, -1.90320522e-01,\n",
    "                         -1.17557538e+00, 6.47376060e-01, -2.21928850e-01, 6.27228469e-02, 6.14576302e-02, 1.23500000e+02]\n",
    "    fraud_example = [4.0600000e+02, -2.3122265e+00, 1.9519920e+00, -1.6098508e+00, 3.9979055e+00, -5.2218789e-01, -1.4265453e+00, -2.5373874e+00,\n",
    "                     1.3916572e+00, -2.7700894e+00, -2.7722721e+00, 3.2020333e+00, -2.8999074e+00, -5.9522188e-01, -4.2892537e+00, 3.8972411e-01, -1.1407472e+00,\n",
    "                     -2.8300557e+00, -1.6822468e-02, 4.1695571e-01, 1.2691055e-01, 5.1723236e-01, -3.5049368e-02, -4.6521106e-01, 3.2019821e-01, 4.4519167e-02,\n",
    "                     1.7783980e-01, 2.6114500e-01, -1.4327587e-01, 0.0000000e+00]\n",
    "    examples = [fraud_example, non_fraud_example]\n",
    "    idx = 1\n",
    "    if random.random() < 0.05:\n",
    "        idx = 0\n",
    "    return ','.join(map(str, examples[idx]))\n",
    "\n",
    "def get_fraud_prediction(data):\n",
    "    sagemaker_endpoint_name = 'fraud-detection-endpoint-c5p18xl'\n",
    "    sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "    tic = time.time()\n",
    "    response = sagemaker_runtime.invoke_endpoint(EndpointName=sagemaker_endpoint_name, ContentType='text/csv',\n",
    "                                                 Body=data)\n",
    "    toc = time.time()\n",
    "    print(\"invocation latency\", toc-tic)\n",
    "    print(response)\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    print(result)\n",
    "    pred = int(result['predictions'][0]['predicted_label'])\n",
    "    return predbajk\n",
    "\n",
    "def postprocess_event(event, pred):\n",
    "    millisecond_regex = r'\\.\\d+'\n",
    "    timestamp = re.sub(millisecond_regex, '', str(datetime.datetime.now()))\n",
    "    source = random.choice(['Mobile', 'Web', 'Store'])\n",
    "    return [timestamp, 'random_id', source, str(pred)]\n",
    "\n",
    "def store_data_prediction(data):\n",
    "    firehose_delivery_stream = 'fraud-detection-firehose-stream'\n",
    "    firehose = boto3.client('firehose', region_name=os.environ['AWS_REGION'])\n",
    "    record = ','.join(data) + '\\n'\n",
    "    response = firehose.put_record(DeliveryStreamName=firehose_delivery_stream, Record={'Data': record})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_endpoint_name = 'fraud-detection-endpoint-c5p18xl'\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tic = time.time()\n",
    "N = 5000\n",
    "skipped = 0\n",
    "warmup_flag = 0\n",
    "for i in range(N):\n",
    "    data = get_data(None, None)\n",
    "    if data == None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    #print(data)    \n",
    "    response = sagemaker_runtime.invoke_endpoint(EndpointName=sagemaker_endpoint_name, ContentType='text/csv', Body=data)\n",
    "    warmup_flag += 1\n",
    "    if warmup_flag == 1:\n",
    "        print(\"1st invocation latency\", time.time() - tic)\n",
    "        tic = time.time()\n",
    "toc = time.time()\n",
    "N = N - skipped\n",
    "print(\"invocation latency\", (toc-tic)/N)\n",
    "print(response)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)\n",
    "pred = int(result['predictions'][0]['predicted_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tic = time.time()\n",
    "skipped = 0\n",
    "warmup_flag = 0\n",
    "for i in range(N):\n",
    "    data = get_data(None, None)\n",
    "    if data == None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    warmup_flag += 1\n",
    "    if warmup_flag == 1:\n",
    "        print(\"1st invocation latency\", time.time() - tic)\n",
    "        tic = time.time()\n",
    "toc = time.time()\n",
    "N = N - skipped\n",
    "print(\"{:10.10f}\".format((toc - tic)/N))\n",
    "#print \"{:10.4f}\".format(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
